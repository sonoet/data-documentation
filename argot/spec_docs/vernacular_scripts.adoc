:toc:
:toclevels: 5
:toc-placement!:

= Handling vernacular script data in Argot
Summary of approach with some examples

toc::[]


== Goals

=== Treat vernacular data in an 880 field as we would the corresponding data in the linked MARC field.
* Do not select a few Argot fields that will get data from 880 fields, as we did in Endeca.
** All vernacular data from the MARC record should display (but script detection should not be needed to make this happen)
** Vernacular data mapping into _indexed_ fields should have script detected and a `lang` element added.
* Reuse the main MARC-to-Argot logic for data in the 880s -- that is, treat an 880 with $6 beginning with 700 like you would a 700 field, except also identify the lang/script of the field

=== Handle vernacular data in non-880 fields, i.e. http://www.loc.gov/marc/bibliographic/ecbdmulti.html#modelb[Model B Multiscript records]

* Model B is a valid method of coding non-Roman script data in a MARC record, according to the MARC standard.
* Currently titles cataloged via Model B are not retrievable at all by title or author if those data are recorded in CJK scripts. Search performance for such titles featuring other scripts is compromised.
* Vendor-provided MARC records (especially SerialsSolutions brief records) often use this model. 
* MARC and its successor(s) for bibliographic description are leaning toward this model and away from the "practices that favored converted Latin-script text over the original script and limited the number of scripts that could be used" (i.e. transliterated data in normal MARC fields, with vernacular recorded in linked 880s).footnote:[See https://www.eventscribe.com/2018/ALA-Annual/fsPopup.asp?Mode=presInfo&PresentationID=352464[New Directions in Non-Latin Script Access]]
* See spreadsheet at https://github.com/trln/data-documentation/blob/master/meta/unc_vernacular.xlsx for data on prevalence Model B non-Roman data in the UNC Catalog

If is is not feasible to detect non-Roman scripts in all MARC fields across the board, we can greatly improve what we have now by focusing on the Argot fields most important for retrieval of known items!footnoteref:[impfields,title_main, names, this_work, included_work], and CJK.footnote:[The segmentation needed for properly indexing CJK causes these records to be unretrievable.] 

=== Priority scripts

According to Duke and UNC, the priority scripts are:

* CJK (Including CJK Unified Ideographs, CJK Compatibility Ideographs, Katakana, Hangul Syllables, Hiragana)
* Cyrillic
* Arabic

=== Anti-goal -- explicitly keeping data values from linked fields together in Solr

In the interest of keeping things as simple as possible (HA!) we are assuming that processing the fields (main MARC and linked 880) in order will give a sufficiently meaningful display that we can live with.

This is actually better than what we are doing in Endeca, where, unless controlled by special sequence indicators, the order of values in a multi-value field defaults to alphabetical order.

For example:

[source]
----
=700  1\$6880-05$aYang, Ziqiong.
=700  1\$6880-06$aMei, Yanfang.
=700  1\$6880-07$aZhang, Manyu.
=700  1\$6880-08$aTo, Johnny.
=710  2\$aTai Seng Video Marketing Inc.
=710  2\$6880-09$aBai jia feng ying ye zhi zuo gong si.
=720  1\$aChing, Siu-Tung.$4drt
=720  1\$aTo, Johnnie.$4drt
=880  1\$6700-05/$1$a楊紫瓊.
=880  1\$6700-06/$1$a梅艷芳.
=880  1\$6700-07/$1$a張曼玉.
=880  1\$6700-08/$1$a杜琪峯.
=880  2\$6710-09/$1$a百嘉峰影業製作公司.
----

.TRLN Discovery display
====

* Yang, Ziqiong.
* Mei, Yanfang.
* Zhang, Manyu.
* To, Johnny.
* Tai Seng Video Marketing Inc.
* Bai jia feng ying ye zhi zuo gong si.
* Ching, Siu-Tung, director.
* To, Johnnie, director.
* 楊紫瓊.
* 梅艷芳.
* 張曼玉.
* 杜琪峯.
* 百嘉峰影業製作公司.

====

Compare with the display in http://search.trln.org/search?id=UNCb3171140[SearchTRLN] and https://search.lib.unc.edu/search?R=UNCb3171140[UNC Endeca] where the fields are all out of order.

== Approach
=== Character set detection logic
The purpose of character set detection in MARC-to-Argot is to ensure the proper language parser is applied to the data in Solr. 

==== Explicitly coded in MARC data
===== Per 880 field
Every 880 field should contain a $6, structured as follows:

`$6830-06(2/r`

830:: Should always be present. MARC tag of field linked to this one. This is a vernacular representation of data in an 830 field
-:: Should always be present. Separates MARC field tag and occurrence number
06:: Should always be present. Occurrence number for linking. This 880 field corresponds to the 830 field that has a $6 beginning with 880-06
(2:: May or may not be present. Graphic character set used in this 880 field, in this case, Basic Hebrew
/r:: May or may not be present. Text directionality code (right-to-left)

*Extract the two characters following `/\d{3}-\d{2}/` and match them using the code translation table below.*

===== Per record
In the section above, note that the graphic character set may be missing from a given 880 field.

Also, if Model B for multiscript records was used, the vernacular data may be in regular MARC fields such as 245 or 100, which lack per-field character set codes.

In these cases, there may be useful information in an 066, which is used to record the character set(s) present in a record.

In general:

* In Model A records (having 880s), we only care about what's in 066$c (alternate script(s))
* In Model B records, we may need to care about 066 $a, $b, _or_ $c

[TIP]
====
In all cases, it's _probably_ safe to get all 066 subfields, throw out any codes that map to Basic Latin or Extended Latin, and set the results in the Traject clipboard for the record, for later reference.
====

[source]
----
=066  \\$c$1
----

In the above record, all 880 fields are expected to be in CJK scripts.

[TIP]
====
When there is an 066 with a single $c value it might be a useful shortcut to use that value and skip the per-880 field character set detection.
====

[CAUTION]
====
Note that 066$c is repeatable. When repeated, 066$c data may be insufficient to identify the language parser that should be applied to a given field. 
====

[source]
----
=066  \\$c(N$c(S$c(Q
----

In the above record, the vernacular title is in Basic Greek, but the rest of the 880s are recorded in Cyrillic.

[TIP]
====
When there is an 066 with multiple $c values, use per-880 graphic character codes if present. If per-880 codes are _not_ present, the 066$c values could be used to apply only the necessary alternative character set detection functions on each 880 field.
====

[source]
----
=066  \\$b(N$c(B$c(3
----

In the above record, most of the main fields are recorded in Basic Cyrillic. However, some are recorded in Extended Latin or Basic Arabic. There are no 880s, and thus no per-field character set codes, present in this record.

[TIP]
====
066 values can be used to limit the alternative character set detection functions needed on the fields in a Model B record
====

===== Code translation

.http://www.loc.gov/marc/specifications/speccharmarc8.html#technique2[Graphic character set code translations]
[%header,cols=2*] 
|===
|Code
|Character set

|$1
|CJK (Chinese, Japanese, Korean (EACC))

|(3
|Basic Arabic

|(4
|Extended Arabic

|(B
|Basic Latin

|(!E
|Extended Latin

|(N
|Basic Cyrillic

|(Q
|Extended Cyrillic

|(S
|Basic Greek

|(2
|Basic Hebrew

|===

==== Why explicit MARC coding is not sufficient
===== The MARC specification hurts us here
As more and more MARC providers and ILSs support UTF-8, we run into the following:

[quote, CHARACTER SETS AND ENCODING OPTIONS: Part 3. Unicode Encoding Environment, http://www.loc.gov/marc/specifications/speccharucs.html ]
____
Field 066 (Character Sets Present) is not used in Unicode-encoded MARC 21 records in the Unicode environment. During conversion of MARC 21 records from MARC-8 encoding to Unicode, field 066 should be deleted.

The subfield $6 script identification code in MARC-8-encoded MARC 21 records identifies MARC-8 character sets, rather than scripts per se; hence the code is irrelevant in the Unicode environment because the character set is always UCS, which has no script identification code value. The script identification code should be dropped from subfield $6 when converting to Unicode from MARC-8 encoding.
____


This means techically properly coded MARC written/stored in Unicode/UTF-8 will lack any explicit coding of the character sets present in the record.

===== Records using Model B may lack any explicit character set coding

There is nothing except the vernacular characters present in the fields themselves to indicate that vernacular characters are present.

==== Alternative character set detection

I leave the best method for doing this to the devs to figure out.

[TIP]
====
As described above in "Why explicit MARC coding is not sufficient," we are going to need to figure this out for some situations, at least for some character sets/scripts we're prioritizing

If we are figuring this out and applying it across-the-board-ish, *does it make sense to skip processing the explicit MARC coding altogether, and just rely on the alternative method?*
====

If the alternative method(s) are so resource-intensive that we need to minimize their use, they ideally should be applied to:

* 880 fields lacking charset codes when there is no 066$c
* 880 fields lacking charset codes when there are multiple 066$c values
* All fields when:
** 066 with non-Latin code(s) present; AND
** No 880s present
* important fieldsfootnoteref:[impfields] when there are no 066 or 880 fields in the record (at the very least, check for CJK-ness)

=== Argot structure
==== Basic idea
Any field value indexed for search may have a `lang` assigned. When `lang` is not assigned, no non-standard language parsing will be applied.

[WARNING]
====
This will require changes to the structure of many already-implemented Argot fields.
====

==== Field-specific examples

===== `title_main`

====== Vernacular in 880

[source]
----
=245  10$6880-02$aUrbilder ;$bBlossoming ; Kalligraphie ; O Mensch, bewein' dein' Sünde gross (Arrangement) : for string quartet /$cToshio Hosokawa.
=880  10$6245-02/{dollar}1$a原像 ;$b開花 ; 書 （カリグラフィー） ほか : 弦楽四重奏のための /$c細川俊夫.
----

[source,ruby]
----
[
{'value'=>'Urbilder ; Blossoming ; Kalligraphie ; O Mensch, bewein\' dein\' Sünde gross (Arrangement) : for string quartet'},
{'value'=>'原像 ;$b開花 ; 書 （カリグラフィー） ほか : 弦楽四重奏のための',
 'lang'=>'cjk'}
]
----

====== Vernacular in 245

[source]
----
=001  \\sseb026776854
=003  \\WaSeSS
=005  \\20170307180154.0
=040  \\$aWaSeSS$beng$cWaSeSS$dWaSeSS
=100  1\$aRekho.
=245  10$a近代日本文学研究の問題点$h[electronic resource] /$cRekho.
=250  \\$a14
=260  \\$a[S.l.] :$b国際日本文化研究センター,$c1992.
=300  \\$a1 online resource
=506  0\$fUnlimited simultaneous users
=588  0\$aTitle from content provider.
=590  \\$aProvider: ERDB Project in Japan (Provisional)
=590  \\$aVendor supplied catalog record.
----

This record http://search.trln.org/search?id=UNCb8821064[exists in SearchTRLN], but http://search.trln.org/search?N=0&Nty=1&Ntk=Title&Ntt=%E8%BF%91%E4%BB%A3%E6%97%A5%E6%9C%AC%E6%96%87%E5%AD%A6%E7%A0%94%E7%A9%B6%E3%81%AE%E5%95%8F%E9%A1%8C%E7%82%B9&sugg=[you cannot find it by searching for the title] because the vernacular data in the 245 isn't recognized/processed as CJK.


[source,ruby]
----
[
{'value'=>'近代日本文学研究の問題点',
 'lang'=>'cjk'}
]
----


