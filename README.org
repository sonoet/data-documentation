#+OPTIONS: ^:nil num:nil title:nil toc:nil
* About the files here
This is currently a mess because it's early in the process. Stuff will be restructured/clarified later...

 - .tsv and .txt files: data documentation/mapping files
   - *IMPORTANT WARNING!*: data files beginning with _ are generated based on other data files. Do not edit them directly, or you will lose your edits.
 - .rb files: ruby scripts that:
   - extract data documentation/mapping files from code, config files, or other data sources
   - manipulate data documentation/mapping files -- combining, summarizing, etc.
 - .org files - notes, textual documentation, tasks, etc.
   - [[https://github.com/trln/data-documentation/blob/master/endeca_data_preparation.org][endeca_data_preparation.org]] - Details on preparation of Endeca-specific data documentation and what lives in the files
   - [[https://github.com/trln/data-documentation/blob/master/update_2017-01-23.org][update_2017-01-23.org]] - notes for Steering Committee meeting
   - [[https://github.com/trln/data-documentation/blob/master/work.org][work.org]] - task planning/tracking
   - [[https://github.com/trln/data-documentation/blob/master/working_notes.org][working_notes.org]] - notes not otherwise organized -- best to ignore
   - [[https://github.com/trln/data-documentation/blob/master/README.org][README.org]] - generates README.md

* Vision for a maintainable, usable suite of data documentation that meets ongoing project needs
It is understood that: 
 - extraction of local institutional data and its tranformation into Argot for submission to the TRLN Discovery application will be a LOCALLY run and controlled process
 - each institution has local practices and requirements preventing a completely shared/coordinated data mapping logic
 - Neither TRLN nor the members of the TRLN Discovery Steering Committee and Implementation Teams have the authority to mandate:
   - use of any tool or approach to locally extract data and map it to Argot
   - participation in shared documentation 

That said, at the 2017-01-23 TRLN Discovery Steering Committee meeting, there was unanimous agreement that all institutions should commit to a shared approach to data documentation and development of a shared, configurable/customizable architecture for mapping local data to Argot. 

This agreement was based on the idea that such a commitment would support: 

** development of an efficient, effective architecture for mapping institutional data into shared Argot format
 - Particularly relevant for the MARC bibliographic data that makes up the vast majority of the shared catalog
 - MARC bibliographic is a well-defined standard data format and its application is largely guided by other standards such as RDA, and broadly shared practices such as those documented in Library of Congress-Program for Cooperative Cataloging Policy Statements (LC-PCC PSs)
 - Thus, the vast majority of data processing/mapping from MARC to Argot should be identical across institutions
 - When new MARC fields/subfields are defined, we should only need to figure out the transformation/mapping of these elements to Argot *once*
 - This can be accomplished by the development of:
   - one main [[http://robotlibrarian.billdueber.com/2015/02/reintroducing-traject/][Traject]] script to handle most of the MARC -> Argot transformation and mapping
   - a configuration file for each institution, which enables:
     - override of transformations specified in main script, i.e. "don't do that"
     - extension of transformations specified in main script, i.e. "do that, but also do this"
     - defining new fully-local transformations

** well-informed development of initial and later-phase features and services
 - The TRLN Discovery Advisory Team has set out the goal of ongoing collaborative development for TRLN Discovery features and services
 - Developers need to understand the data on top of which they'll be building features and services
 - The shared data documentation will serve as a reference for this work, allowing developers to easily see where data definitions/practices are the same and where they differ across institutions

** shared resolution of data processing/display/behavior issues
Currently in Endeca: 
 - Ayse, Dawn, or I may be stumped on a data problem.
 - We can and do ask each other for help/ideas, but it's difficult to help when none of us knows how the others' local data gets processed for inclusion in Endeca
 - When there is a problem with a MARC-based shared record, only one institution can do anything about it

Complete, correct centralized data documentation will increase our ability to share troubleshooting work. 

When problems cross the data/transformation applications/UI code lines, relevant developers will again have all the information they need to assist.

This will also allow more responsibility for onboarding new Data Team staff (regardless of institution) to fall to the overall Data Team, rather than being completely institution-specific. This is a large win in terms of the succession planning problems we've encountered with Endeca.

It will also allow metadata experts at institutions to make informed recommendations about data model/behavior improvements or adjustments over time.

** empowering library staff (and by extension, library users) to better understand how their catalog works
 - The idea is to present this data documentation in such a way that you do not have to be a developer or Data Team expert to use it. 
 - It will provide easy answers to the most common types of data questions that arise, such as: 
  - Public services staff: What does Publisher search actually search (MARC fields/subfields from the catalog record, metadata elements from digital collection record, etc)?
  - Tech services/IT/metadata staff: If I record this data in a given MARC tag/subfield, or output it from a repository to given DC/MODS field, will it be searchable and/or displayed in the public catalog?
  - Staff, superusers: I see this data in the ILS client/classic view/WorldCat/MARC-or-'librarian view', but I don't see it in the public catalog record. Why?
  - Everyone: Why did this record come up in my search? or Why didn't this record come up in my search?
  - Everyone: Why are my search results in this order?

** maintenance of the data documentation with minimum human effort
The TRLN Endeca data model spreadsheet became out-of-date and partly incorrect (and was of limited utility) because:
   - Only one person was authorized to update it
   - Updating it was an extra step for someone to do. Changes had to made to the Endeca configuration, and also in this document, which has no actual relation to how Endeca works
   - It reflected only updates approved by the Endeca Operations Committee, and therefore did not contain all the information necessary to make real decisions or answer questions

Further, the history of the document was not maintained. 

We are planning a very different approach for the TRLN Discovery data documentation, guided by the following assumptions/ideas: 
 - Most of the data documentation's required content is explicitly defined in config files and code across the application: Traject scripts, configs, Solr schema, Blacklight/Argon configs, etc.
 - As much as possible, the documentation should be generated/extracted from these sources so that:
   - it always accurately reflects the actual application configuration
   - no one has to manually update separate documentation
 - Conversely, in some cases, underlying config/code files could be updated based on changes to human-readable documentation. For example, imagine if changing the indexes for a property in the TRLN Endeca data model spreadsheet had updated all the relevant XML files in the pipeline.
 - Everything under version control

There will no doubt be some pieces of the documentation which need to be hand-curated/updated, but in general, the bulk of the extra work to support this tool involves: 
 - working out the design/implementation of the documentation format/app(s)

We are already making significant progress on this work. 

* Resources and notes
 - [[http://docwiki.embarcadero.com/ERStudioDA/XE7/en/Documenting_Data_Extraction,_Transformation,_and_Load][Concept and workflow of *data lineage* for documenting ETL]]
