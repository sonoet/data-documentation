<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org9a0c58b">There is no one "data model" for this project. I don't have a draft of one for you.</a>
<ul>
<li><a href="#orga75e79d">We are working toward a suite of data documentation, not a data model</a></li>
<li><a href="#org8b4b514">It's too early to be nailing down our data documentation in a formal, nice-to-look at way</a></li>
</ul>
</li>
<li><a href="#org1aa1ffd">Current data-related priorities &amp; status</a></li>
<li><a href="#org80afe97">Challenge/blocker for all aspects of data work and documentation from start to finish</a></li>
<li><a href="#orgbbc9e0c">Vision for a maintainable, usable suite of data documentation that meets ongoing project needs</a>
<ul>
<li><a href="#org00ffe80">further feature development</a></li>
<li><a href="#orgde3b35e">issue resolution</a></li>
<li><a href="#org6f8badd">maintenance</a></li>
<li><a href="#org67caf68">empowering library staff (and by extension, library users) to better understand how their catalog works</a></li>
</ul>
</li>
</ul>
</div>
</div>


<a id="org9a0c58b"></a>

# There is no one "data model" for this project. I don't have a draft of one for you.


<a id="orga75e79d"></a>

## We are working toward a suite of data documentation, not a data model

We are dealing with at least 4 different types of data models (bold), with at least 3 transformation/mapping processes (marked by &#x2013;>):

-   **Original data sources** - MARC bib + attached record data, EAD, non-MARC shared record and ICE data XML feeds (each unique), digital collection data (mapping requirements may vary per collection). Each of these sources is defined/modeled differently
-   &#x2013;> Traject/other transformation process (partly, if not completely institution- and data-source-specific)&#x2013;>
-   **Argot**
-   &#x2013;> centrally processed within Spofford &#x2013;>
-   **Solr schema**
-   &#x2013;> transformed within Solr &#x2013;>
-   **actual Solr index data fields upon which Argon data behavior with depend**

**We need a plan for documenting all of the above and more; a single/simple spreadsheet "data model" is not a good way to do this.**

Endeca data processing was similarly complex, but the nature of the system made it more difficult to clearly see/understand all the steps. 

The single spreadsheet "data model" approach taken with Endeca was ultimately insufficient. Its shortcomings included:

-   spreadsheet was incomplete, and in some cases incorrect
-   spreadsheet data poorly structured for meeting the data needs of Endeca  maintenance, ongoing development, and staff/user training. The needs I am referring to here are laid out in [Vision for a maintainable, usable suite of data documentation that meets ongoing project needs](#orgbbc9e0c)

The above factors meant that, in order to confidently know how any data was handled in Endea, one needed to: 

-   use clunky Java Reference Implementation interface
-   access and understand extract/transformation code, whether local or embedded in Endeca as Java MARC manipulator
-   access and understand Endeca configuration and UI code

Experts in the code are not generally experts in the metadata and how it should work, and vice versa.

This made it extremely difficult to handle problem-solving, cross-training, succession-planning, and gathering broader input on data decisions affecting Endeca.


<a id="org8b4b514"></a>

## It's too early to be nailing down our data documentation in a formal, nice-to-look at way

**Data & Infrastructure Team chairs concur: Argon feature development should drive data decisionmaking, not the other way around:**

-   The architecture of this project much greater flexibility for iterative development than we had with Endeca, which required much more overhead for dealing with any changes to data model than our approach will.
-   No one feels expert enough with Solr + Blacklight+ all our data to pre-devise a set of data specifications/mappings from original data sources -> Argot -> Solr -> Argon.
-   It's too early to nail down our data specifications, which is one reason why production of "a data model" as a piece of data documentation has not yet been produced


<a id="org1aa1ffd"></a>

# Current data-related priorities & status

See [Work tracking page](https://github.com/trln/data-documentation/blob/master/work.md).


<a id="org80afe97"></a>

# Challenge/blocker for all aspects of data work and documentation from start to finish

This project has stated goals to, as much as possible, cooperate and share processes and code to minimize repeated work and complexity. 

However, the project and staff working on it have no mandate to really agree on or enforce agreed-upon shared practices for their institutions in the future. So the basic working assumption those of us working on this have is: "Everybody's going to end up doing their own thing their own way."

From the data side, because MARC bib and EAD are standard formats, much of our data transformation **code** and documentation could be shared/standardized, but we know important and significant parts of it will be highly localized. 

Committing to shared/consistent data transformation practices where possible, and centralized documentation of cross-institutional data differences, would afford us some long-term benefits: 

-   Most work necessary to implement changes required by changes to bibliographic standards would only need to be made once
-   Ability of all players to understand (and contribute to solutions to) issues with shared records visible in their catalogs, but hosted in someone else's ILS
-   Support troubleshooting data issues cross-institutionally
-   Benefits for succession planning across institutions (avoiding the Derek Rodriguez or "what if Kristina leaves" situation) &#x2013; data expertise from other institutions can fill many gaps while there are vacancies at others, and the team could help new staff responsible for data extract/transformation get up to speed on all the post-extract parts. Each institution's main concern would be ensuring coverage for data extract processes.
-   Much of what is discussed under [Vision for a maintainable, usable suite of data documentation that meets ongoing project needs](#orgbbc9e0c)
-   etc

Organizing this collaborative work would require particular steps going forward: 

-   transformation code designed with  main standard transformtion instructions and separate institutional override/localization configs
-   all/most data documentation centrally hosted and structured/designed to reflect multi-institution practice

If there is no committment to full ongoing collaboration on this, these extra (but not particularly onerous) steps are unnecessary, and some things can be simplified. In this case: 

-   Luke and I are committed to sharing UNC's data extract and transformation code and documentation as models for other institutions to adapt/reference/copy
-   The Argot format will be the one required point of agreement across institutions, and its documentation should be centralized.
    -   Argot is source-format independent, so the Argot documentation itself will not include mappings from MARC tags, EAD, Dublin Core into Argot. Those would vary per institution and need to be documented locally.
    -   UNC would commit to share its mappings from source data formats into Argot for other institutions to reference/adapt
-   Documentation of the following data models and processes should also be documented centrally, and in such a way that this information can be easily mashed-up with local data documentation. 
    1.  Argot -> Solr ingest format processing/mapping done by Spofford
    2.  Solr schema data model (high level, declarative instructions for indexing the data)
    3.  (?) Solr schema -> Actual Solr/Lucene index fields that occurs during indexing
    4.  (? - potentially localized) Argon data model (what Solr index fields are displayed, how they are labeled, whether they are searchable, drive facets, etc)

Given what I hear off the record, I have moved forward assuming we do not have the committment to full collaboration on data. However, it would be good to have a decision on the record about how we want to move forward on this. 


<a id="orgbbc9e0c"></a>

# Vision for a maintainable, usable suite of data documentation that meets ongoing project needs


<a id="org00ffe80"></a>

## further feature development

-   clear
-   complete
-   correct


<a id="orgde3b35e"></a>

## issue resolution

-   collaborative
-   version controlled


<a id="org6f8badd"></a>

## maintenance

-   easy to update, current
    -   automate whatever can be automated (i.e. leverage extraction of human-readable documentation from code, config files)
    -   follow standards for open online data formatting and sharing
-   supports periodic review of data needs as original data source standards evolve


<a id="org67caf68"></a>

## empowering library staff (and by extension, library users) to better understand how their catalog works

-   accessible
-   provides simple ways to arrive at answers to the common types of data questions that arise such as: 
    -   Public services staff: What does Publisher search actually search (MARC fields/subfields from the catalog record, metadata elements from digital collection record, etc)?
    -   Tech services/IT/metadata staff: If I record this data in a given MARC tag/subfield, or output it from a repository to given DC/MODS field, will it be searchable and/or displayed in the public catalog?
    -   Staff, superusers: I see this data in the ILS client/classic view/WorldCat/MARC-or-'librarian view', but I don't see it in the public catalog record. Why?
    -   Everyone: Why did this record come up in my search? or Why didn't this record come up in my search?
    -   Everyone: Why are my search results in this order?

